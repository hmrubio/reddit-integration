{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Primero, importamos las librerías necesarias:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import ast\n",
    "import configparser\n",
    "import fileinput\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import requests.auth\n",
    "import json\n",
    "from datetime import datetime\n",
    "from requests import Response"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Luego, configuramos variables globales para utilizar en el ETL:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "config_file_parser = configparser.ConfigParser()\n",
    "config_file_parser.read('../main.py.ini')\n",
    "\n",
    "general_config = config_file_parser['GENERAL']\n",
    "login_config = config_file_parser['LOGIN']\n",
    "search_endpoint = config_file_parser['SEARCH_ENDPOINT']\n",
    "subreddit_endpoint = config_file_parser['SUBREDDIT_ENDPOINT']\n",
    "comment_endpoint = config_file_parser['COMMENT_ENDPOINT']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se crean los siguientes métodos auxiliares que se utilizarán en la recuperación de subreddits:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_authentication():\n",
    "    username = login_config['USERNAME']\n",
    "    password = login_config['PASSWORD']\n",
    "    return requests.auth.HTTPBasicAuth(username, password)\n",
    "\n",
    "\n",
    "def launch_request(name: str, api: requests, url, auth, **params) -> Response:\n",
    "    response_recover = None\n",
    "\n",
    "    if general_config.getboolean(\"USE_RESTORABLE_REQUEST\"):\n",
    "        response_recover = recover_success_request(name)\n",
    "\n",
    "    if response_recover is None:\n",
    "        new_response = api.get(\n",
    "            url,\n",
    "            params=params,\n",
    "            auth=auth\n",
    "        )\n",
    "\n",
    "        if new_response.status_code == 200:\n",
    "            save_success_request(new_response, name)\n",
    "            response_recover = new_response\n",
    "        else:\n",
    "            raise Exception(\"It was no possible to recover a useful request. \"\n",
    "                            \"Reject because too many requests.\")\n",
    "\n",
    "    return response_recover\n",
    "\n",
    "\n",
    "def show_compilation_started() -> datetime:\n",
    "    datetime_start = datetime.now()\n",
    "    print(\"-\" * 60,\n",
    "          f\"\\nProceso de recopilación iniciado: {datetime_start.strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    return datetime_start\n",
    "\n",
    "\n",
    "def show_compilation_finished(datetime_start: datetime):\n",
    "    datetime_end = datetime.now()\n",
    "\n",
    "    print(f\"\\nProceso terminado: {datetime_end.strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "    print(f\"Duración del proceso: {datetime_end - datetime_start} horas/minutos/segundos\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "def update_filesize_message(filesize, limit):\n",
    "    sys.stdout.write(\n",
    "        f\"\\rTamaño actual del archivo: {filesize / 1000} kb | Cantidad de subreddits: {limit}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_success_request(response_request: Response, temp_filename: str):\n",
    "    temporary_path = f\"temp/{temp_filename}.pickle\"\n",
    "    with open(temporary_path, \"wb\") as file:\n",
    "        pickle.dump(response_request, file)\n",
    "\n",
    "\n",
    "def recover_success_request(temp_filename: str) -> Response:\n",
    "    temporary_path = f\"temp/{temp_filename}.pickle\"\n",
    "    if os.path.exists(temporary_path):\n",
    "        with open(temporary_path, \"rb\") as file:\n",
    "            return pickle.load(file)\n",
    "\n",
    "\n",
    "def get_name_from_subreddit_url(url: str) -> str:\n",
    "    url = url.removeprefix(\"/r/\")\n",
    "    url = url.removesuffix(\"/.json\")\n",
    "    return url\n",
    "\n",
    "\n",
    "def create_directory_if_not_exists(relative_path: str):\n",
    "    if not os.path.exists(relative_path):\n",
    "        os.makedirs(relative_path, exist_ok=False)\n",
    "\n",
    "\n",
    "def create_necessary_directories():\n",
    "    create_directory_if_not_exists(\"temp\")\n",
    "    create_directory_if_not_exists(\"temp/subreddits_comments\")\n",
    "    create_directory_if_not_exists(\"temp/comments\")\n",
    "\n",
    "    create_directory_if_not_exists(\"data\")\n",
    "    create_directory_if_not_exists(\"data/subreddits_comments\")\n",
    "    create_directory_if_not_exists(\"data/comments\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos el siguiente método que recuperará en formato binario, JSON y txt los subreddits disponibles para trabajar con comentarios:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_subreddits(query, sort, limit):\n",
    "    url = f\"{general_config['URL_BASE']}/{search_endpoint['URL']}\"\n",
    "    auth = get_authentication()\n",
    "    params = {\n",
    "        'q': query,\n",
    "        'sort': sort,\n",
    "        'limit': limit,\n",
    "        'raw_json': 1\n",
    "    }\n",
    "\n",
    "    response = launch_request(\"subreddits_search\", requests, url, auth, **params)\n",
    "\n",
    "    path_subreddits = search_endpoint['PATH_SAVING']\n",
    "    path_subreddits_urls = search_endpoint['PATH_SAVING_URLS']\n",
    "\n",
    "    with open(path_subreddits, \"w\", encoding=\"utf-8\") as file, \\\n",
    "            open(path_subreddits_urls, \"w\", encoding=\"utf-8\") as file_urls:\n",
    "        try:\n",
    "            datetime_start = show_compilation_started()\n",
    "            subreddits = response.json()['data']['children']\n",
    "            for subreddit in subreddits:\n",
    "                json.dump(subreddit, file, ensure_ascii=False, indent=None)\n",
    "                file.write(\"\\n\")\n",
    "                update_filesize_message(file.tell(), limit)\n",
    "\n",
    "                url = subreddit[\"data\"][\"url\"]\n",
    "                file_urls.write(f\"{url}\\n\")\n",
    "\n",
    "            show_compilation_finished(datetime_start)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al ser llamado, recupera en la carpeta \"data\" la información requerida.\n",
    "\n",
    "Luego, es necesario recuperar los comentarios disponibles de cada subreddit. Para ello, se crean dos métodos que buscan tal fin. A continuación, los definimos:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def get_comment_ids_from_subreddits():\n",
    "    path_subreddits_urls = search_endpoint['PATH_SAVING_URLS']\n",
    "    with open(path_subreddits_urls, \"r\", encoding=\"utf-8\") as file_urls:\n",
    "        string = subreddit_endpoint['URL_FORMAT']\n",
    "        datetime_start = show_compilation_started()\n",
    "        for url in file_urls:\n",
    "            url = string.format(subreddit=url).replace(\"\\n\", \"\")\n",
    "            try:\n",
    "                get_comment_ids_from_subreddit(url)\n",
    "            except:\n",
    "                print(f\"The next URL was skipped because an error: {url}\")\n",
    "                continue\n",
    "        show_compilation_finished(datetime_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def get_comment_ids_from_subreddit(url: str):\n",
    "    name_subreddit = get_name_from_subreddit_url(url)\n",
    "\n",
    "    url = f\"{general_config['URL_BASE']}{url}\"\n",
    "    auth = get_authentication()\n",
    "    params = {\n",
    "        'raw_json': 1\n",
    "    }\n",
    "\n",
    "    response = launch_request(f\"subreddits_comments/{name_subreddit}\", requests, url, auth, **params)\n",
    "    if response is None:\n",
    "        return\n",
    "\n",
    "    path_subreddit_comments = subreddit_endpoint[\"PATH_COMMENTS_ID\"] \\\n",
    "        .format(subreddit=name_subreddit)\n",
    "\n",
    "    with open(path_subreddit_comments, \"w\", encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            comments = response.json()['data']['children']\n",
    "            for comment in comments:\n",
    "                comment_id = comment[\"data\"][\"id\"]\n",
    "                file.write(f\"{name_subreddit, comment_id}\\n\")\n",
    "        except Exception as e:\n",
    "            print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ambos métodos tienen el objetivo de recuperar los ids de comentarios disponibles. Luego, se recuperará el cuerpo de los mismos. Para esto último, se genera un nuevo método:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def get_comments_from_ids():\n",
    "    path_comments = comment_endpoint['PATH_COMMENTS_ID']\n",
    "\n",
    "    files = [os.path.join(path_comments, file_name) for file_name in os.listdir(path_comments) if\n",
    "             os.path.isfile(os.path.join(path_comments, file_name))]\n",
    "\n",
    "    with fileinput.input(files=files) as unique_file:\n",
    "        datetime_start = show_compilation_started()\n",
    "        for line in unique_file:\n",
    "            subreddit, comment_id = ast.literal_eval(line)\n",
    "            try:\n",
    "                get_comments_info_from_subreddit(subreddit, comment_id)\n",
    "            except:\n",
    "                print(f\"The next ID was skipped because an error: \"\n",
    "                      f\"{subreddit}_{comment_id}\")\n",
    "                continue\n",
    "        show_compilation_finished(datetime_start)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def get_comments_info_from_subreddit(subreddit: str, comment_id: str):\n",
    "    url = f\"{general_config['URL_BASE']}/\" \\\n",
    "          f\"{comment_endpoint['URL_FORMAT'].format(subreddit=subreddit, comment_id=comment_id)}\"\n",
    "    auth = get_authentication()\n",
    "    params = {\n",
    "        'raw_json': 1\n",
    "    }\n",
    "\n",
    "    response = launch_request(f\"comments/{subreddit}_{comment_id}\", requests, url, auth, **params)\n",
    "    if response is None:\n",
    "        return\n",
    "\n",
    "    path_subreddit_comments = comment_endpoint[\"PATH_COMMENT_FORMAT\"] \\\n",
    "        .format(subreddit=subreddit, comment_id=comment_id)\n",
    "    with open(path_subreddit_comments, \"w\", encoding=\"utf-8\") as file:\n",
    "        try:\n",
    "            comment_info = response.json()\n",
    "            json.dump(comment_info, file, indent=2)\n",
    "        except Exception as e:\n",
    "            print(e)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, se llaman a tres métodos claves (de los creados con anterioridad) para completar el proceso. Esto lo hacemos en el siguiente bloque de código. Cabe aclarar que el mensaje __The next ID was skipped because an error__ es ocasionado por un rechazo de parte de Reddit al envío reiterado de requests. Esto se corregirá en próximos entregables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------ \n",
      "Proceso de recopilación iniciado: 05/02/2024 22:04:29\n",
      "Tamaño actual del archivo: 65.909 kb | Cantidad de subreddits: 10\n",
      "Proceso terminado: 05/02/2024 22:04:29\n",
      "Duración del proceso: 0:00:00.009973 horas/minutos/segundos\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------ \n",
      "Proceso de recopilación iniciado: 05/02/2024 22:04:29\n",
      "The next URL was skipped because an error: /r/Republica_Argentina/.json\n",
      "The next URL was skipped because an error: /r/uruguay/.json\n",
      "The next URL was skipped because an error: /r/InvertirEnArgentina/.json\n",
      "\n",
      "Proceso terminado: 05/02/2024 22:04:34\n",
      "Duración del proceso: 0:00:04.826471 horas/minutos/segundos\n",
      "------------------------------------------------------------\n",
      "------------------------------------------------------------ \n",
      "Proceso de recopilación iniciado: 05/02/2024 22:04:34\n",
      "The next ID was skipped because an error: AirsoftEnArgentina_191zf5s\n",
      "The next ID was skipped because an error: AirsoftEnArgentina_195qygi\n",
      "The next ID was skipped because an error: AirsoftEnArgentina_192kao8\n",
      "The next ID was skipped because an error: AirsoftEnArgentina_1922vhy\n",
      "The next ID was skipped because an error: Argaming_19dd6dl\n",
      "The next ID was skipped because an error: Argaming_1ai8ojc\n",
      "The next ID was skipped because an error: Argaming_1aiw52f\n",
      "The next ID was skipped because an error: Argaming_1aiuakv\n",
      "The next ID was skipped because an error: Argaming_1ai0veq\n",
      "The next ID was skipped because an error: Argaming_1aig8mz\n",
      "The next ID was skipped because an error: Argaming_1aitf2y\n",
      "The next ID was skipped because an error: Argaming_1ahzkbt\n",
      "The next ID was skipped because an error: Argaming_1aii65c\n",
      "The next ID was skipped because an error: Argaming_1ahuz59\n",
      "The next ID was skipped because an error: Argaming_1ahufjs\n",
      "The next ID was skipped because an error: Argaming_1aho423\n",
      "The next ID was skipped because an error: Argaming_1ahc6vg\n",
      "The next ID was skipped because an error: Argaming_1ahcp2n\n",
      "The next ID was skipped because an error: Argaming_1ai3ntz\n",
      "The next ID was skipped because an error: Argaming_1ahlz5g\n",
      "The next ID was skipped because an error: Argaming_1ahzgsx\n",
      "The next ID was skipped because an error: Argaming_1ah7fhs\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_snm6ja\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_sq6po9\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_sona9g\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_son5b5\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_son3md\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_sonbar\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_snpoog\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_snpql0\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_snpldq\n",
      "The next ID was skipped because an error: ArgentinaEnDatos_snm8ng\n",
      "The next ID was skipped because an error: argentina_1ajc71v\n",
      "The next ID was skipped because an error: argentina_1ajc75n\n",
      "The next ID was skipped because an error: argentina_1ajnq4c\n",
      "The next ID was skipped because an error: argentina_1ajstzg\n",
      "The next ID was skipped because an error: argentina_1ajposm\n",
      "The next ID was skipped because an error: argentina_1ajhaft\n",
      "The next ID was skipped because an error: argentina_1ajvkas\n",
      "The next ID was skipped because an error: argentina_1ajsk0u\n",
      "The next ID was skipped because an error: argentina_1ajov5o\n",
      "The next ID was skipped because an error: argentina_1ajpt0u\n",
      "The next ID was skipped because an error: argentina_1ajtp2b\n",
      "The next ID was skipped because an error: argentina_1ajpt9p\n",
      "The next ID was skipped because an error: argentina_1ajugje\n",
      "The next ID was skipped because an error: argentina_1ajlaag\n",
      "The next ID was skipped because an error: argentina_1ajjgvg\n",
      "The next ID was skipped because an error: argentina_1ajk2eu\n",
      "The next ID was skipped because an error: argentina_1ajp5in\n",
      "The next ID was skipped because an error: argentina_1ajrzxg\n",
      "The next ID was skipped because an error: argentina_1ajh9pp\n",
      "The next ID was skipped because an error: argentina_1ajv5jo\n",
      "The next ID was skipped because an error: argentina_1ajwe4f\n",
      "The next ID was skipped because an error: argentina_1ajsjzg\n",
      "The next ID was skipped because an error: argentina_1ajho19\n",
      "The next ID was skipped because an error: ComicsEnArgentina_11k0jhp\n",
      "The next ID was skipped because an error: ComicsEnArgentina_11k0k1j\n",
      "The next ID was skipped because an error: ComicsEnArgentina_11kyb9y\n",
      "The next ID was skipped because an error: ComicsEnArgentina_11k0u0j\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_e676te\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_b6c3sp\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_a1julj\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_8rdu16\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_8nlo7b\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_77ee6h\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_71ll8x\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_5q89ax\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_5nvo38\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_5l12r5\n",
      "The next ID was skipped because an error: LaMusicaEnArgentina_4ws133\n",
      "The next ID was skipped because an error: RepublicaArgentina_199q87j\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajkrhe\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajqlu1\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajhhc1\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajswn9\n",
      "The next ID was skipped because an error: RepublicaArgentina_1aiyxvk\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajnxlz\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajqgtq\n",
      "The next ID was skipped because an error: RepublicaArgentina_1aj7nuy\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajq2gk\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajwhd1\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajnoiy\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajgysj\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajiuw6\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajit15\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajvnc7\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajgwg0\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajmu95\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajmqpa\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajken4\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajikot\n",
      "The next ID was skipped because an error: RepublicaArgentina_1aircp2\n",
      "The next ID was skipped because an error: RepublicaArgentina_1aivjs7\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajlnfs\n",
      "The next ID was skipped because an error: RepublicaArgentina_1ajkda2\n",
      "\n",
      "Proceso terminado: 05/02/2024 22:05:18\n",
      "Duración del proceso: 0:00:44.366451 horas/minutos/segundos\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "create_necessary_directories()\n",
    "\n",
    "get_subreddits(search_endpoint['QUERY'], search_endpoint['SORT'], search_endpoint['LIMIT'])\n",
    "get_comment_ids_from_subreddits()\n",
    "get_comments_from_ids()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, estos datos se grabarán en las tres tablas relacionadas de author, subreddit y comment. El código para crearlas se encuentra en la carpeta ../instructions_database/database/. También, es posible visibilizarlas en Amazon Redshift, esquema rubiomatias2_coderhose.\n",
    "\n",
    "Se adiciona imagen para ver la composición de tablas:\n",
    "![](resources/entregable_1_tabla_comment.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
